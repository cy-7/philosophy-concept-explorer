# Configuration Guide (é…ç½®æŒ‡å—)

## ğŸ“ è·¯å¾„é…ç½®

### 1. æ¨¡å‹æ–‡ä»¶è·¯å¾„

åœ¨ `backend/config.py` ä¸­ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š

```python
# æ¨¡å‹é…ç½®
MODEL_CONFIG = {
    "model_path": "YOUR_MODEL_PATH_HERE",  # ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    # ä¾‹å¦‚: "C:/path/to/your/model.gguf"
}

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODEL_CONFIG = {
    "model_path": "YOUR_MODEL_PATH_HERE",  # ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    # ä¾‹å¦‚: "C:/path/to/your/model.gguf"
}
```

**æ”¯æŒçš„æ¨¡å‹æ ¼å¼**:
- Qwen-7B-Chat: `qwen-7b-chat-f16.gguf`, `qwen-7b-chat-q4_0.gguf`
- LLaMA-2: `llama-2-7b-chat.gguf`, `llama-2-13b-chat.gguf`
- å…¶ä»–GGUFæ ¼å¼æ¨¡å‹

### 2. LLaMA.cpp è·¯å¾„

åœ¨å¯åŠ¨è„šæœ¬ä¸­ä¿®æ”¹LLaMA.cppè·¯å¾„ï¼š

**æ–¹æ³•1: ä¿®æ”¹å¯åŠ¨è„šæœ¬**
ç¼–è¾‘ `start_llm_system.bat` æˆ– `start_services_only.bat`ï¼Œæ‰¾åˆ°ä»¥ä¸‹è¡Œï¼š
```batch
echo cd YOUR_LLAMA_CPP_PATH
echo .\llama-server.exe -m YOUR_MODEL_FILE -c 2048 --host 0.0.0.0 --port 8080
```

å°†å…¶æ›¿æ¢ä¸ºå®é™…è·¯å¾„ï¼š
```batch
cd C:\path\to\your\llama.cpp\build\bin\Release
.\llama-server.exe -m C:\path\to\your\model.gguf -c 2048 --host 0.0.0.0 --port 8080
```

**æ–¹æ³•2: ä½¿ç”¨ç¯å¢ƒå˜é‡**
åˆ›å»º `.env` æ–‡ä»¶ï¼š
```env
LLAMA_CPP_PATH=C:\path\to\your\llama.cpp\build\bin\Release
MODEL_PATH=C:\path\to\your\model.gguf
```

### 3. é¡¹ç›®è·¯å¾„

é¡¹ç›®è·¯å¾„ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œæ— éœ€ä¿®æ”¹ï¼š
```batch
# åç«¯æœåŠ¡
cd /d %~dp0 && python run_backend.py

# å‰ç«¯æœåŠ¡  
cd /d %~dp0\frontend && npm start
```

## âš™ï¸ å…¶ä»–é…ç½®

### GPUé…ç½®

å¦‚éœ€å¯ç”¨GPUåŠ é€Ÿï¼Œåœ¨ `backend/config.py` ä¸­ä¿®æ”¹ï¼š
```python
LOCAL_MODEL_CONFIG = {
    "gpu_enabled": True,    # å¯ç”¨GPU
    "gpu_layers": 35,       # GPUå±‚æ•°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    "main_gpu": 0,          # ä¸»GPUç´¢å¼•
}
```

### ç«¯å£é…ç½®

é»˜è®¤ç«¯å£é…ç½®ï¼š
- **LLaMA.cppæœåŠ¡å™¨**: 8080
- **åç«¯APIæœåŠ¡**: 8000  
- **å‰ç«¯å¼€å‘æœåŠ¡å™¨**: 3000

å¦‚éœ€ä¿®æ”¹ï¼Œåœ¨ç›¸åº”é…ç½®æ–‡ä»¶ä¸­æ›´æ–°ã€‚

## ğŸ”§ é…ç½®ç¤ºä¾‹

### å®Œæ•´é…ç½®ç¤ºä¾‹

```python
# backend/config.py
LOCAL_MODEL_CONFIG = {
    "enabled": True,
    "host": "localhost",
    "port": 8080,
    "model_path": "D:/models/qwen-7b-chat-f16.gguf",  # ä½ çš„æ¨¡å‹è·¯å¾„
    "api_type": "openai_compatible",
    "max_tokens": 2000,
    "temperature": 0.7,
    "context_length": 2048,
    "gpu_enabled": False,  # CPUæ¨¡å¼
    "gpu_layers": 35,
    "main_gpu": 0,
}
```

### å¯åŠ¨è„šæœ¬ç¤ºä¾‹

```batch
# start_llm_system.bat
cd D:\llama.cpp\build\bin\Release
.\llama-server.exe -m D:\models\qwen-7b-chat-f16.gguf -c 2048 --host 0.0.0.0 --port 8080
```

## ğŸ“‹ é…ç½®æ£€æŸ¥æ¸…å•

å¯åŠ¨å‰è¯·ç¡®è®¤ï¼š

- [ ] æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
- [ ] LLaMA.cppå·²ç¼–è¯‘
- [ ] ç«¯å£æœªè¢«å ç”¨
- [ ] Pythonå’ŒNode.jså·²å®‰è£…
- [ ] ä¾èµ–åŒ…å·²å®‰è£…

## ğŸš¨ å¸¸è§é…ç½®é”™è¯¯

### 1. è·¯å¾„é”™è¯¯
```
Error: Model file not found!
```
**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥ `backend/config.py` ä¸­çš„æ¨¡å‹è·¯å¾„

### 2. LLaMA.cppæœªç¼–è¯‘
```
Error: LLaMA.cpp not compiled!
```
**è§£å†³æ–¹æ¡ˆ**: æŒ‰ç…§READMEç¼–è¯‘LLaMA.cpp

### 3. ç«¯å£è¢«å ç”¨
```
Port 8000 is already in use
```
**è§£å†³æ–¹æ¡ˆ**: è„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†ï¼Œæˆ–æ‰‹åŠ¨å…³é—­å ç”¨ç«¯å£çš„ç¨‹åº

## ğŸ’¡ é…ç½®æŠ€å·§

1. **ä½¿ç”¨ç»å¯¹è·¯å¾„**: é¿å…ç›¸å¯¹è·¯å¾„å¯èƒ½çš„é—®é¢˜
2. **è·¯å¾„åˆ†éš”ç¬¦**: Windowsä½¿ç”¨ `\` æˆ– `/`ï¼ŒLinux/Macä½¿ç”¨ `/`
3. **ç¯å¢ƒå˜é‡**: ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿè·¯å¾„ä¿¡æ¯
4. **é…ç½®æ–‡ä»¶**: å°†é…ç½®é›†ä¸­åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ç®¡ç†

## ğŸ” é…ç½®éªŒè¯

é…ç½®å®Œæˆåï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ï¼š

```bash
# æ£€æŸ¥Pythoné…ç½®
python -c "from backend.config import LOCAL_MODEL_CONFIG; print(LOCAL_MODEL_CONFIG)"

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
python -c "import os; print(os.path.exists('YOUR_MODEL_PATH'))"
```

---

**æ³¨æ„**: é¦–æ¬¡ä½¿ç”¨å‰å¿…é¡»å®Œæˆè·¯å¾„é…ç½®ï¼Œå¦åˆ™ç³»ç»Ÿæ— æ³•æ­£å¸¸è¿è¡Œã€‚
